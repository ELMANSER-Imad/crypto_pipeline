import subprocess
import happybase
import pyspark.sql.functions as F
from pyspark.sql import SparkSession
from datetime import datetime
import logging
import traceback

# Configuration du logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Initialiser une session Spark
spark = SparkSession.builder \
    .appName("Parquet to HBase") \
    .config("spark.sql.shuffle.partitions", "200") \
    .config("spark.executor.memory", "4g") \
    .getOrCreate()

# Connexion √† HBase
try:
    connection = happybase.Connection('localhost')
    table = connection.table('crypto_prices')
    logging.info("‚úÖ Connexion √† HBase r√©ussie")
except Exception as e:
    logging.error(f"‚ùå Erreur de connexion √† HBase : {e}")
    exit(1)

# Chemin de base des donn√©es HDFS
base_path = "/user/etudiant/crypto/processed/YYYY=2025"

# Fonction pour lister les fichiers/dossiers dans HDFS
def list_hdfs_files(path):
    """ Liste les fichiers ou dossiers pr√©sents dans HDFS √† l'aide de `hdfs dfs -ls` """
    try:
        result = subprocess.run(["hdfs", "dfs", "-ls", path], capture_output=True, text=True)
        if result.returncode != 0:
            logging.warning(f"‚ö†Ô∏è Aucun fichier trouv√© dans {path}")
            return []
        
        lines = result.stdout.strip().split("\n")[1:]  # Ignorer la premi√®re ligne (total X)
        return [line.split()[-1] for line in lines]  # Extraire le chemin du fichier/dossier
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de la lecture de {path} : {e}")
        return []

# Fonction pour traiter un fichier Parquet et l'ins√©rer dans HBase
def process_parquet_file(file_path, date):
    try:
        # V√©rifier si le fichier existe dans HDFS
        result = subprocess.run(["hdfs", "dfs", "-test", "-e", file_path], capture_output=True)
        if result.returncode != 0:
            logging.warning(f"‚ö†Ô∏è Fichier non trouv√© : {file_path}")
            return

        # Lire le fichier Parquet depuis HDFS
        df = spark.read.parquet(file_path)

        # V√©rifier que les colonnes n√©cessaires sont pr√©sentes
        required_columns = {"coin_id", "price", "volume"}
        if not required_columns.issubset(set(df.columns)):
            logging.warning(f"‚ö†Ô∏è Fichier {file_path} ne contient pas toutes les colonnes requises {required_columns}")
            return
        
        # Afficher le nom du fichier et le nombre de lignes
        logging.info(f"üìÇ Traitement du fichier : {file_path}")
        logging.info(f"üìä Nombre de lignes : {df.count()}")

        # Transformer les donn√©es (agr√©gation)
        aggregated_df = df.groupBy("coin_id").agg(
            F.min("price").alias("min_price"),
            F.max("price").alias("max_price"),
            F.avg("price").alias("avg_price"),
            F.sum("volume").alias("total_volume")
        )

        # Convertir le DataFrame Spark en Pandas pour l'insertion dans HBase
        pandas_df = aggregated_df.toPandas()

        # V√©rifier si le DataFrame est vide
        if pandas_df.empty:
            logging.warning(f"‚ö†Ô∏è Aucune donn√©e √† ins√©rer pour {file_path}")
            return

        # Ins√©rer les donn√©es dans HBase avec un batch pour optimiser
        with table.batch(batch_size=1000) as batch:
            for _, row in pandas_df.iterrows():
                row_key = f"{row['coin_id']}_{date}".encode()
                batch.put(row_key, {
                    b'stats:price_min': str(row['min_price']).encode(),
                    b'stats:price_max': str(row['max_price']).encode(),
                    b'stats:price_avg': str(row['avg_price']).encode(),
                    b'stats:volume_sum': str(row['total_volume']).encode()
                })

        logging.info(f"‚úÖ Donn√©es ins√©r√©es dans HBase pour {file_path}")

    except Exception as e:
        logging.error(f"‚ùå Erreur lors du traitement de {file_path} : {e}")
        traceback.print_exc()  # Afficher la stack trace pour le d√©bogage

# V√©rifier si le r√©pertoire de base existe
if not list_hdfs_files(base_path):
    logging.error(f"‚ùå Aucun fichier trouv√© dans le r√©pertoire de base : {base_path}")
    exit(1)

# Parcourir les mois (MM=01, MM=02, etc.)
for month_path in list_hdfs_files(base_path):
    month = month_path.split("=")[-1]  # Extraire MM

    # Parcourir les jours (DD=01, DD=02, etc.)
    for day_path in list_hdfs_files(month_path):
        day = day_path.split("=")[-1]  # Extraire DD

        # R√©cup√©rer la date (YYYY-MM-DD)
        try:
            date = datetime.strptime(f"2025-{month}-{day}", "%Y-%m-%d").strftime("%Y-%m-%d")
        except ValueError:
            logging.error(f"‚ùå Format de date invalide pour {month}-{day}")
            continue

        # Parcourir les fichiers Parquet dans HDFS
        for file_path in list_hdfs_files(day_path):
            logging.info(f"üìÑ Fichier trouv√© : {file_path}")
            if file_path.endswith(".parquet"):
                process_parquet_file(file_path, date)

# Fermer la connexion HBase et arr√™ter Spark
try:
    connection.close()
    spark.stop()
    logging.info("‚úÖ Fin du traitement")
except Exception as e:
    logging.error(f"‚ùå Erreur lors de la fermeture des ressources : {e}")